# PPO for Glucose Control in Type 1 Diabetes

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**A from-scratch implementation of Proximal Policy Optimisation (PPO) for adaptive insulin dosing in Type 1 Diabetes.**

This project demonstrates how reinforcement learning can be applied to biomedical control problems, specifically maintaining blood glucose within a safe therapeutic range (70-180 mg/dL) whilst avoiding dangerous hypoglycaemia.

![PPO Clipped Objective](figures/04_clipped_objective.png)

---

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [The RL Problem](#the-rl-problem)
- [Usage](#usage)
- [Experiments](#experiments)
- [Results](#results)
- [References](#references)

---

## Overview

### Why PPO for Glucose Control?

Traditional insulin delivery relies on fixed protocols or simple rule-based systems. However, glucose dynamics are:
- **Highly variable** between patients
- **Non-linear** and time-varying
- **Affected by meals**, exercise, stress, and sleep

PPO offers several advantages for this problem:
1. **Stability**: The clipped surrogate objective prevents destructive policy updates
2. **Safety**: Conservative updates are crucial when actions affect patient health
3. **Adaptability**: Learns patient-specific patterns without explicit modelling
4. **Sample efficiency**: Reuses experience data safely through clipping

### Clinical Context

This implementation uses reward functions and evaluation metrics from clinical diabetes literature:

| Metric | Target | Description |
|--------|--------|-------------|
| Time in Range (TIR) | >70% | Percentage of time glucose is 70-180 mg/dL |
| Time Below Range (TBR) | <4% | Percentage of time glucose is <70 mg/dL |
| Time Above Range (TAR) | <25% | Percentage of time glucose is >180 mg/dL |

---

## Key Features

- **Complete PPO implementation** from scratch in PyTorch
- **Realistic glucose-insulin dynamics** based on the Bergman Minimal Model
- **Continuous action space** for precise insulin dosing
- **Multi-objective reward function** balancing efficacy, safety, and stability
- **Comprehensive experiments** comparing epsilon values and baselines
- **Publication-ready visualisations** for learning curves, clinical metrics, and PPO diagnostics
- **Baseline controllers** for comparison (Fixed Basal, Basal-Bolus, PID)

---

## Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ppo-glucose-control.git
cd ppo-glucose-control
```

2. **Create a virtual environment** (recommended)
```bash
python -m venv ppo_env
source ppo_env/bin/activate  # On Windows: ppo_env\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Dependencies
```
torch>=2.0.0
gymnasium>=0.29.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scipy>=1.10.0
pandas>=2.0.0
tqdm>=4.65.0
pyyaml>=6.0
```

---

## Quick Start

### Option 1: One-command training and visualisation
```bash
python run.py quickstart --timesteps 50000
```

This will:
- Train a PPO agent for 50,000 timesteps
- Generate all visualisations
- Save results to `./results/quickstart_TIMESTAMP/`

### Option 2: Step-by-step
```bash
# 1. Run environment demo
python run.py demo

# 2. Train agent
python run.py train --timesteps 100000 --seed 42

# 3. Generate visualisations
python run.py visualise --results-dir ./results/YOUR_EXPERIMENT/

# 4. Run baseline comparison
python run.py evaluate --model ./results/YOUR_EXPERIMENT/models/final_model.pt
```

### Option 3: Run individual files
```bash
# Test environment
python environment.py

# Test networks
python networks.py

# Test PPO agent
python ppo_agent.py

# Run training
python train.py --timesteps 100000
```

---

## Project Structure
```
ppo-glucose-control/
â”‚
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚
â”œâ”€â”€ environment.py            # Glucose-insulin simulation environment
â”‚   â”œâ”€â”€ GlucoseInsulinEnv     # Gymnasium environment class
â”‚   â”œâ”€â”€ Bergman model         # Glucose-insulin dynamics
â”‚   â”œâ”€â”€ Reward function       # Multi-objective reward
â”‚   â””â”€â”€ Patient generation    # Inter-patient variability
â”‚
â”œâ”€â”€ networks.py               # Neural network architectures
â”‚   â”œâ”€â”€ ContinuousActorCritic # Gaussian policy + value function
â”‚   â””â”€â”€ SeparateActorCritic   # Alternative architecture
â”‚
â”œâ”€â”€ ppo_agent.py              # PPO algorithm implementation
â”‚   â”œâ”€â”€ PPOAgent              # Main agent class
â”‚   â”œâ”€â”€ RolloutBuffer         # Experience storage
â”‚   â”œâ”€â”€ GAE computation       # Advantage estimation
â”‚   â””â”€â”€ Clipped loss          # PPO objective
â”‚
â”œâ”€â”€ train.py                  # Training loop
â”‚   â””â”€â”€ PPOTrainer            # Training management
â”‚
â”œâ”€â”€ evaluate.py               # Evaluation and baselines
â”‚   â”œâ”€â”€ FixedBasalController  # Constant insulin rate
â”‚   â”œâ”€â”€ BasalBolusController  # Standard therapy
â”‚   â”œâ”€â”€ RuleBasedController   # Simple feedback control
â”‚   â””â”€â”€ PIDController         # Classical control
â”‚
â”œâ”€â”€ visualise.py              # Plotting functions
â”‚   â””â”€â”€ PPOVisualiser         # All visualisation methods
â”‚
â”œâ”€â”€ experiments.py            # Hyperparameter experiments
â”‚   â””â”€â”€ PPOExperiment         # Experiment management
â”‚
â”œâ”€â”€ run.py                    # Main entry point (CLI)
â”‚
â””â”€â”€ results/                  # Output directory
    â”œâ”€â”€ models/               # Saved model weights
    â””â”€â”€ figures/              # Generated plots
```

---

## The RL Problem

### State Space (8 dimensions)

| Index | Variable | Description | Normalisation |
|-------|----------|-------------|---------------|
| 0 | G_t | Current glucose (mg/dL) | Ã· 200 |
| 1 | G_{t-1} | Previous glucose | Ã· 200 |
| 2 | G_{t-2} | Glucose 2 steps ago | Ã· 200 |
| 3 | I_t | Plasma insulin (mU/L) | Ã· 50 |
| 4 | IOB_t | Insulin-on-board (U) | Ã· 10 |
| 5 | CHO_t | Recent carbohydrate (g) | Ã· 100 |
| 6 | t_meal | Time since last meal (h) | Ã· 6 |
| 7 | weight | Patient weight (kg) | Ã· 100 |

### Action Space (continuous)
```
a_t âˆˆ [0, 5] U/hr (insulin infusion rate)
```

### Reward Function

Based on Zhu et al. (2020) and clinical guidelines:

| Glucose Zone | Range (mg/dL) | Reward |
|--------------|---------------|--------|
| Severe hypoglycaemia | < 54 | -4.0 |
| Hypoglycaemia | 54-70 | -2.0 |
| **Target range** | **70-180** | **+1.0 to +1.5** |
| Mild hyperglycaemia | 180-250 | -0.5 |
| Hyperglycaemia | 250-400 | -1.0 |
| Severe hyperglycaemia | > 400 | -2.0 |

Additional penalties:
- **Rate of change**: -0.1 Ã— |Î”G| / 50 (stability)
- **Insulin usage**: -0.01 Ã— IOB / 10 (efficiency)

### Episode Structure

- **Duration**: 24 hours (288 steps at 5-minute intervals)
- **Meals**: Breakfast (7:00, 50g), Lunch (12:00, 70g), Dinner (18:00, 80g)
- **Termination**: Severe hypoglycaemia (G < 40 mg/dL) or time limit

---

## Usage

### Command Line Interface
```bash
# General help
python run.py --help

# Command-specific help
python run.py train --help
```

### Training
```bash
# Basic training
python run.py train --timesteps 100000

# With custom parameters
python run.py train \
    --timesteps 200000 \
    --steps-per-update 4096 \
    --seed 123 \
    --name my_experiment
```

### Evaluation
```bash
# Evaluate trained model
python run.py evaluate --model results/exp/models/final_model.pt --episodes 20

# Compare against baselines (no model needed)
python run.py evaluate --episodes 10
```

### Experiments
```bash
# Epsilon comparison (key experiment!)
python run.py experiment --type epsilon --values 0.1,0.2,0.3 --seeds 42,123,456

# Reward weight comparison
python run.py experiment --type reward --seeds 42,123

# Baseline comparison
python run.py experiment --type baseline
```

### Visualisation
```bash
# Generate all plots from training results
python run.py visualise --results-dir results/my_experiment/

# Generate educational PPO figure only
python run.py visualise --epsilon 0.2
```

---

## Experiments

### Experiment 1: Epsilon (Îµ) Comparison

The clipping parameter Îµ controls how much the policy can change in a single update.
```bash
python run.py experiment --type epsilon --values 0.1,0.2,0.3 --timesteps 50000
```

**Expected findings:**
- Îµ = 0.1: Conservative, stable but slow learning
- Îµ = 0.2: **Optimal balance** (recommended default)
- Îµ = 0.3: Fast learning but unstable

![Epsilon Comparison](figures/epsilon_comparison.png)

### Experiment 2: Baseline Comparison

Compare PPO against traditional control methods:
```bash
python run.py experiment --type baseline
```

**Baselines:**
1. **Fixed Basal (1.0 U/hr)**: Constant insulin delivery
2. **Basal-Bolus**: Standard clinical protocol with meal boluses
3. **Rule-Based**: Proportional control based on glucose level
4. **PID Controller**: Classical control theory approach

### Experiment 3: Reward Weight Comparison

Test different balances of efficacy vs safety:
```bash
python run.py experiment --type reward
```

---

## Results

### Training Performance

After 100,000 timesteps of training:

| Metric | Value |
|--------|-------|
| Mean Episode Reward | ~150-200 |
| Time in Range | 65-80% |
| Time Below Range | 2-5% |
| Severe Hypoglycaemia Events | <1% |

### Comparison with Baselines

| Controller | Reward | TIR (%) | TBR (%) |
|------------|--------|---------|---------|
| Fixed Basal | -50 | 45 | 8 |
| Basal-Bolus | +20 | 55 | 5 |
| Rule-Based | +50 | 58 | 6 |
| PID | +80 | 62 | 4 |
| **PPO Agent** | **+150** | **75** | **3** |

### Generated Visualisations

1. **Learning Curves** (`01_learning_curves.png`)
2. **Training Metrics** (`02_training_metrics.png`)
3. **Clipping Behaviour** (`03_clipping_behaviour.png`)
4. **PPO Objective Explanation** (`04_clipped_objective.png`)
5. **Glucose Profiles** (`05_glucose_profile.png`)
6. **Clinical Comparison** (`06_clinical_comparison.png`)
7. **Epsilon Experiment** (`07_epsilon_comparison.png`)
8. **Summary Dashboard** (`08_summary_dashboard.png`)

---

## Understanding PPO

### The Core Insight

Vanilla policy gradients suffer from a critical flaw: **each batch of experience can only be used once**. Reusing data causes the policy to diverge catastrophically.

PPO solves this with a simple clipping mechanism:
```
L_CLIP(Î¸) = E[min(r(Î¸)Â·A, clip(r(Î¸), 1-Îµ, 1+Îµ)Â·A)]

where r(Î¸) = Ï€_new(a|s) / Ï€_old(a|s)
```

### Why Clipping Works

1. **For good actions (A > 0)**: Clipping at 1+Îµ prevents over-optimisation
2. **For bad actions (A < 0)**: Clipping at 1-Îµ prevents complete elimination

This creates an **implicit trust region** without complex constrained optimisation.

### Why This Matters for Medical Applications

In glucose control, a policy that changes too dramatically could:
- Deliver excessive insulin â†’ dangerous hypoglycaemia
- Withhold necessary insulin â†’ prolonged hyperglycaemia

PPO's conservative updates ensure **safe, gradual policy improvement**.

---

## Extending This Project

### Adding New Environments
```python
# Create a new environment following the Gymnasium interface
class MyBiomedicalEnv(gym.Env):
    def __init__(self, ...):
        self.observation_space = spaces.Box(...)
        self.action_space = spaces.Box(...)
    
    def reset(self, seed=None):
        # Return initial state
        return state, info
    
    def step(self, action):
        # Simulate dynamics
        return next_state, reward, terminated, truncated, info
```

### Modifying the Reward Function

Edit the `_compute_reward` method in `environment.py`:
```python
def _compute_reward(self) -> float:
    G = self.glucose
    
    # Customise zone boundaries and penalties
    if G < YOUR_HYPO_THRESHOLD:
        base_reward = YOUR_PENALTY
    # ... etc
```

### Hyperparameter Tuning

Key parameters to tune in `ppo_agent.py`:

| Parameter | Default | Range | Effect |
|-----------|---------|-------|--------|
| `epsilon` | 0.2 | 0.1-0.3 | Clipping aggressiveness |
| `lr` | 3e-4 | 1e-4 to 1e-3 | Learning speed |
| `gamma` | 0.99 | 0.95-0.999 | Future reward weighting |
| `gae_lambda` | 0.95 | 0.9-0.99 | Advantage estimation bias-variance |
| `update_epochs` | 10 | 3-20 | Data reuse per batch |
| `entropy_coef` | 0.01 | 0.001-0.1 | Exploration bonus |

---

## References

### PPO Algorithm

1. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). **Proximal Policy Optimization Algorithms**. arXiv:1707.06347.

2. Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). **Trust Region Policy Optimization**. ICML.

### Glucose Control & RL in Healthcare

3. Zhu, T., Li, K., Herrero, P., & Georgiou, P. (2020). **Deep Reinforcement Learning for Personalized Treatment Recommendation**. IEEE JBHI.

4. Fox, I., Lee, J., Pop-Busui, R., & Wiens, J. (2020). **Deep Reinforcement Learning for Closed-Loop Blood Glucose Control**. MLHC.

5. Gottesman, O., Johansson, F., Komorowski, M., et al. (2019). **Guidelines for Reinforcement Learning in Healthcare**. Nature Medicine.

### Pharmacokinetic Modelling

6. Bergman, R. N., Ider, Y. Z., Bowden, C. R., & Cobelli, C. (1979). **Quantitative Estimation of Insulin Sensitivity**. American Journal of Physiology.

---

## Citation

If you use this code in your research, please cite:
```bibtex
@misc{ppo-glucose-control,
  author = {Your Name},
  title = {PPO for Glucose Control in Type 1 Diabetes},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/yourusername/ppo-glucose-control}
}
```

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgements

- LLM for assistance with code development
- OpenAI for the original PPO paper
- The diabetes research community for clinical guidelines and reward function design

---

## Contact

For questions or suggestions, please open an issue or contact [nmokunola@icloud.com].

